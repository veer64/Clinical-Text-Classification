{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "656ce42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r ../requirements.txt\n",
    "# !pip install mlxtend\n",
    "# !pip install imblearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from nltk.corpus import wordnet\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.manifold import TSNE\n",
    "from typing import Union\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.model_selection import train_test_split\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from math import * \n",
    "from collections import *\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import accuracy_score\n",
    "import numpy as np\n",
    "import itertools\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae54ad04",
   "metadata": {},
   "source": [
    "# Data Extraction module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5db01a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def _load_dataframe(\n",
    "#     df: Union[str, pd.DataFrame],\n",
    "#     storage_options: Union[str, dict] = \"\",\n",
    "#     delimiter= ','\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     Load all the required files for creating the output schema.\n",
    "#     Args:\n",
    "#         :param df: Path to the file for reading\n",
    "#         :param storage_options: storage account details\n",
    "#     Returns: \n",
    "#         a dataframe for the required file/ table\n",
    "#     \"\"\"\n",
    "\n",
    "#     if isinstance(df, pd.DataFrame):\n",
    "#         return df\n",
    "\n",
    "#     else:\n",
    "#         df_ = str(df)\n",
    "#         if storage_options == \"\":\n",
    "#             if df_.endswith(\".csv\"):\n",
    "#                 df = pd.read_csv(df_,delimiter=delimiter)\n",
    "#             elif df_.endswith(\".parquet\"):\n",
    "#                 df = pd.read_parquet(df_, engine=\"pyarrow\")\n",
    "#         else:\n",
    "#             if df_.endswith(\".csv\"):\n",
    "#                 df = pd.read_csv(df_, storage_options=storage_options)\n",
    "#             elif df_.endswith(\".parquet\"):\n",
    "#                 df = pd.read_parquet(\n",
    "#                     df_, engine=\"pyarrow\", storage_options=storage_options\n",
    "#                 )\n",
    "#         return df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b56b6f3",
   "metadata": {},
   "source": [
    "# EDA module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2ec8361b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_null_values(df: Union[str, pd.DataFrame]):\n",
    "    \"\"\"\n",
    "    Compute null values in the dataset\n",
    "    Args:\n",
    "        :param dataset_path: Dataset Path to the file for reading\n",
    "    Returns: \n",
    "        percent of null values per columns\n",
    "    \"\"\"\n",
    "\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    missing_value_df = pd.DataFrame({'column_name': df.columns,\n",
    "                                     'percent_missing': percent_missing})\n",
    "    missing_value_df.sort_values('percent_missing', inplace=True)\n",
    "    missing_value_df.reset_index(drop=True, inplace=True)\n",
    "    return missing_value_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "112aa991",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_top_ten_categories(df):    \n",
    "    # Display the top 10 specialties most frequently repeated in the text\n",
    "    # The top 10 specialties will be most likely to be predicted since the model will be trained mainly on them\n",
    "    fig, ax = plt.subplots(figsize = (18,8)) # set size of figure\n",
    "    specialty = df['medical_specialty'].value_counts()\n",
    "    specialty = specialty[:10,]\n",
    "    g = sns.barplot(specialty.index, specialty.values, alpha=0.8)\n",
    "\n",
    "    plt.title(\"Top 10 Specialties in Medical Transcriptions\", fontsize=15)\n",
    "    plt.ylabel(\"Frequency\", fontsize=15)\n",
    "    plt.xlabel(\"Top 10 Specialties out of 40 total\", fontsize=15)\n",
    "\n",
    "    g.set_xticklabels(g.get_xticklabels(), rotation=45, fontsize=17)\n",
    "    plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feeab662",
   "metadata": {},
   "source": [
    "# Text Preprocessing  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "807ed9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# transcription whose length is greater than 50 are selected \n",
    "def filter_categories_by_threshold(data_categories, threshold):\n",
    "    filtered_data_categories = data_categories.filter(lambda x:x.shape[0] > 50)\n",
    "    final_data_categories = filtered_data_categories.groupby(filtered_data_categories['medical_specialty'])\n",
    "    i=1\n",
    "    print('============Reduced Categories ======================')\n",
    "    for catName,dataCategory in final_data_categories:\n",
    "        print('Cat:'+str(i)+' '+catName + ' : '+ str(len(dataCategory)) )\n",
    "        i = i+1\n",
    "\n",
    "    print('============ Reduced Categories ======================')\n",
    "    return filtered_data_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "331c7ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Stopwords and punctuation removal \n",
    "def clean_text(text): \n",
    "    stop_words = ['abc','abcd','cm','ml','mmhg', 'wa', 'patient', 'procedure', 'history','room','ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during','my','on', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than','abcd','abcd general','abcd general hospital','abcd hospital','abdomen abdomen','social history','hospital','history history','patient']\n",
    "    pat = r'\\b(?:{})\\b'.format('|'.join(stop_words))\n",
    "    text = text.replace(pat, '')\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text1 = ''.join([w for w in text if not w.isdigit()]) \n",
    "    REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "    #BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "    \n",
    "    text2 = text1.lower()\n",
    "    text2 = REPLACE_BY_SPACE_RE.sub('', text2) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "    #text2 = BAD_SYMBOLS_RE.sub('', text2)\n",
    "    return text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ed02d58e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_text(text):\n",
    "    wordlist=[]\n",
    "    lemmatizer = WordNetLemmatizer() \n",
    "    sentences=sent_tokenize(text)\n",
    "    \n",
    "#     intial_sentences= sentences[0:1]\n",
    "#     final_sentences = sentences[len(sentences)-2: len(sentences)-1]\n",
    "    \n",
    "#     for sentence in intial_sentences:\n",
    "#         words=word_tokenize(sentence)\n",
    "#         for word in words:\n",
    "#             wordlist.append(lemmatizer.lemmatize(word))\n",
    "    for sentence in sentences:\n",
    "        words=word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            wordlist.append(lemmatizer.lemmatize(word))       \n",
    "    return ' '.join(wordlist) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "49b7fac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorization(data, final_list=None):\n",
    "    vectorizer = TfidfVectorizer(analyzer='word', stop_words='english',ngram_range=(1,1), max_df=0.75, use_idf=True, smooth_idf=True, max_features=1000)\n",
    "    tfIdfMat  = vectorizer.fit_transform(data['transcription'].tolist() )\n",
    "    feature_names = vectorizer.get_feature_names_out() # if you get an error just delete _out \n",
    "    stop_words = ['abc','abcd','cm','ml','mmhg', 'wa', 'patient', 'procedure', 'history','room','ourselves', 'hers', 'between', 'yourself', 'but', 'again', 'there', 'about', 'once', 'during','my','on', 'out', 'very', 'having', 'with', 'they', 'own', 'an', 'be', 'some', 'for', 'do', 'its', 'yours', 'such', 'into', 'of', 'most', 'itself', 'other', 'off', 'is', 's', 'am', 'or', 'who', 'as', 'from', 'him', 'each', 'the', 'themselves', 'until', 'below', 'are', 'we', 'these', 'your', 'his', 'through', 'don', 'me', 'were', 'her', 'more', 'himself', 'this', 'down', 'should', 'our', 'their', 'while', 'above', 'both', 'up', 'to', 'ours', 'had', 'she', 'all', 'when', 'at', 'any', 'before', 'them', 'same', 'and', 'been', 'have', 'in', 'will', 'on', 'does', 'yourselves', 'then', 'that', 'because', 'what', 'over', 'why', 'so', 'can', 'did', 'now', 'under', 'he', 'you', 'herself', 'has', 'just', 'where', 'too', 'only', 'myself', 'which', 'those', 'i', 'after', 'few', 'whom', 't', 'being', 'if', 'theirs', 'my', 'against', 'a', 'by', 'doing', 'it', 'how', 'further', 'was', 'here', 'than','abcd','abcd general','abcd general hospital','abcd hospital','abdomen abdomen','social history','hospital','history history','patient']\n",
    "    if final_list:\n",
    "        feature_names = list(filter(lambda i: i not in stop_words, final_list))\n",
    "    else:\n",
    "        feature_names = list(filter(lambda i: i not in stop_words, feature_names))\n",
    "    return feature_names, tfIdfMat, vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4643501c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes):\n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax= fig.add_subplot(1,1,1)\n",
    "    sns.heatmap(cm, annot=True, cmap=\"Greens\",ax = ax,fmt='g'); #annot=True to annotate cells\n",
    "\n",
    "    # labels, title and ticks\n",
    "    ax.set_xlabel('Predicted labels', fontsize=18);\n",
    "    ax.set_ylabel('True labels', fontsize=18); \n",
    "    ax.set_title('Confusion Matrix', fontsize=18); \n",
    "    ax.xaxis.set_ticklabels(classes); \n",
    "    ax.yaxis.set_ticklabels(classes);\n",
    "    plt.setp(ax.get_yticklabels(), rotation=30, horizontalalignment='right', fontsize=18)\n",
    "    plt.setp(ax.get_xticklabels(), rotation=30, horizontalalignment='right', fontsize=18)     \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c7520d",
   "metadata": {},
   "source": [
    "# Lemmatizing the List "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c0acc63d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_words(words):\n",
    "    lemmas = []\n",
    "    for word in words:\n",
    "        synsets = wordnet.synsets(word)\n",
    "        if synsets:\n",
    "            lemma = synsets[0].lemmas()[0].name()\n",
    "        else:\n",
    "            lemma = word\n",
    "        lemmas.append(lemma)\n",
    "    return lemmas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b43e45d",
   "metadata": {},
   "source": [
    "# Removing Duplicates "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d9b7650e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(lst):\n",
    "    return list(set(lst))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28381f73",
   "metadata": {},
   "source": [
    "# word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f003e275",
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(word):\n",
    "    # Count the number of characters in each word.\n",
    "    count_characters = Counter(word)\n",
    "    # Gets the set of characters and calculates the \"length\" of the vector.\n",
    "    set_characters = set(count_characters)\n",
    "    length = sqrt(sum(c*c for c in count_characters.values()))\n",
    "    return count_characters, set_characters, length, word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a075ea40",
   "metadata": {},
   "source": [
    "# Cosine Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2cc004a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vector1, vector2, ndigits):\n",
    "    \n",
    "    # Get the common characters between the two character sets\n",
    "    common_characters = vector1[1].intersection(vector2[1])\n",
    "    # Sum of the product of each intersection character.\n",
    "    product_summation = sum(vector1[0][character] * vector2[0][character] for character in common_characters)\n",
    "    # Gets the length of each vector from the word2vec output.\n",
    "    length = vector1[2] * vector2[2]\n",
    "    # Calculates cosine similarity and rounds the value to ndigits decimal places.\n",
    "    if length == 0:\n",
    "        # Set value to 0 if word is empty.\n",
    "        similarity = 0\n",
    "    else:\n",
    "        similarity = round(product_summation/length, ndigits)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e8423e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(full_names_list, similarity_threshold, ndigits):\n",
    "    # Initiate an empty list to store results.\n",
    "    results_list = []\n",
    "    # Apply word2vec function to each name and store them in a list.\n",
    "    vector_list = [word2vec(str(i)) for i in full_names_list]\n",
    "    # Two loops to compare each vector with another vector only once.\n",
    "    for i in range(len(vector_list)):\n",
    "        # Get first vector\n",
    "        vector1 = vector_list[i]\n",
    "        for j in range(i+1, len(vector_list)):\n",
    "            # Get the next vector\n",
    "            vector2 = vector_list[j]\n",
    "            # Calculate cosine similarity\n",
    "            similarity_score = cosine_similarity(vector1, vector2, ndigits)\n",
    "            # Append to results list if similarity score is between 1 and the threshold.\n",
    "            # Note that scores of 1 can be ignored here if we want to exclude people with the same name.\n",
    "            if 1 >= similarity_score >= similarity_threshold:\n",
    "                results_list.append([vector1[3], vector2[3], similarity_score])\n",
    "            else:\n",
    "                pass\n",
    "    # Convert list to dataframe.\n",
    "    results_df = pd.DataFrame(results_list)\n",
    "    if len(results_df) != 0:\n",
    "        results_df.columns = ['full_name', 'comparison_name', 'similarity_score']\n",
    "    else:\n",
    "    # Can add error here if there's no results to return if desired.\n",
    "        pass\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02ea9d6",
   "metadata": {},
   "source": [
    "# Jacard Similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "528ca28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard_similarity(list_, threshold=0):\n",
    "    result = []\n",
    "    for x, y in itertools.combinations(list_, 2):\n",
    "        intersection_cardinality = len(set.intersection(*[set(x), set(y)]))\n",
    "        union_cardinality = len(set.union(*[set(x), set(y)]))\n",
    "        jaccard_similarity = intersection_cardinality / float(union_cardinality)\n",
    "        if jaccard_similarity >= threshold:\n",
    "            result.append((x, y, jaccard_similarity))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a990e444",
   "metadata": {},
   "source": [
    "# Logistic Regression Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b0889a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_logistic_regression(X_train, y_train, X_test, logistic_regression_params):\n",
    "    clf = LogisticRegression(**logistic_regression_params).fit(X_train, y_train)\n",
    "    y_pred= clf.predict(X_test)\n",
    "    return y_pred, clf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1a89519",
   "metadata": {},
   "source": [
    "# SVM Modelling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "3f56e589",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def svm_classification(X_train, y_train, X_test, y_test, svm_params):\n",
    "    \n",
    "    SV = svm.SVC(**svm_params).fit(X_train, y_train)\n",
    "    \n",
    "    # Predict the labels of test set\n",
    "    y_pred1 = SV.predict(X_test)\n",
    "\n",
    "    return y_pred1, SV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b8abdc",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e3000a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_random_forest(X_train, y_train, X_test, y_test, rf_params):\n",
    "    # create a random forest classifier with the specified number of estimators and maximum depth\n",
    "    rf = RandomForestClassifier(**rf_params).fit(X_train, y_train)\n",
    "    \n",
    "    y_pred2 = rf.predict(X_test)\n",
    "    \n",
    "    # return the trained classifier\n",
    "    return y_pred2, rf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a340be06",
   "metadata": {},
   "source": [
    "# Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c56b00f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_gbdt(X_train, y_train, X_test, y_test, gbdt_params):\n",
    "    gbdt_clf = GradientBoostingClassifier(**gbdt_params).fit(X_train, y_train)\n",
    "    y_pred3 = gbdt_clf.predict(X_test)\n",
    "    return y_pred3, gbdt_clf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8914ae52",
   "metadata": {},
   "source": [
    "# Categorical Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "fbb77fea",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_catboost(X_train, y_train, X_test, y_test, catboost_params):\n",
    "    cat = CatBoostClassifier(**catboost_params)\n",
    "    cat.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=100)\n",
    "    y_pred4 = cat.predict(X_test)\n",
    "    return y_pred4, cat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "045f7583",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
